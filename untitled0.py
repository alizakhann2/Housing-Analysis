# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17S9u4h-hjgEsoa1cReTITVRZjHk-6PWR
"""

print("hello world")









import pandas as pd

df  = pd.read_csv('Housing.csv')

"""# New Section"""

df.head()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

sns.lmplot(x='area',y='price',data = df , ci=None)

sns.lmplot(x='stories',y='price',data = df , ci=None)

sns.kdeplot(x='area', data=df)

sns.kdeplot(x='stories', data=df)

sns.lmplot(x='parking',y='price',data = df , ci=None)

#df = df.drop(columns=['bedrooms','bathrooms'])
df.head()
len('13300000')

# SEPERATING FEATURES AND TARGET

# We want to predict the price, so we set y as the price column
y = df['price']

# We remove the price columns from the dataframe, and keep the rest and save it as X
X_data = df.drop(columns = ['price'])

# CLEANING OUR DATA

# Converting yes/no values to 1 and 0
# semi-furnished has been given a value of 0.5

X_data['mainroad'] = X_data['mainroad'].map({'yes': 1, 'no': 0})
X_data['guestroom'] = X_data['guestroom'].map({'yes': 1, 'no': 0})
X_data['basement'] = X_data['basement'].map({'yes': 1, 'no': 0})
X_data['hotwaterheating'] = X_data['hotwaterheating'].map({'yes': 1, 'no': 0})
X_data['airconditioning'] = X_data['airconditioning'].map({'yes': 1, 'no': 0})
X_data['prefarea'] = X_data['prefarea'].map({'yes': 1, 'no': 0})
X_data['furnishingstatus'] = X_data['furnishingstatus'].map({'furnished': 1, 'unfurnished': 0, 'semi-furnished': 0.5})
X_data.head()

# Splitting the data into training/testing sets

X = X_data
y = y



from sklearn.model_selection import train_test_split


# We will keep 80% of the data for training purposes
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error

potential_estimators = [i for i in range(100, 5100, 100)] # [100, 200, 300, 400, .... , 5000]
print(potential_estimators)

# Getting the best value for the n_estimators value

def errorchecker(potential_estimators, X_train, X_test, y_train, y_test):
  best = 0
  best_mae = 100000000000000000
  for i in potential_estimators:
    model = XGBRegressor( n_estimators = i, learning_rate = 0.05, early_stopping_rounds=5)
    model.fit(X_train, y_train,
             eval_set=[(X_test, y_test)], verbose=False)
    preds = model.predict(X_test)
    mae = mean_absolute_error(preds, y_test)
    if mae < best_mae:
      best_mae = mae
      best = i
  print(mae)
  return i

best_estimators = errorchecker(potential_estimators, X_train, X_test, y_train, y_test)
print(best_estimators)

# Building the model with the value we got from the function
model = XGBRegressor(n_estimators = best_estimators, learning_rate = 0.05, early_stopping_rounds=5)

# fitting the model
model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)

# making our prediction
preds = model.predict(X_test)

from sklearn.metrics import mean_absolute_percentage_error
# calculating the mean percentage error
mpe = mean_absolute_percentage_error(preds, y_test)
print(mpe)

# As you can see we only had an error of 20 percent using the above defined model
# When tested on testing data

model.score(X,y)

model.intercept_

model.coef_

model.predict([[6000,3]])

model.predict([[7420,4]])

model.predict([[7420,3]])

